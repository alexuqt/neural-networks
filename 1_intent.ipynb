{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3tLaCe5yPsII6QcHjaqHm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexuqt/neural-networks/blob/main/1_intent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPoHqGD-6p-L"
      },
      "outputs": [],
      "source": [
        "!pip install snntorch --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import snntorch as snn\n",
        "from snntorch import spikeplot as splt\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "# ---\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "dtype = torch.long\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# data preparation\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data.dataset import random_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from google.colab import drive\n",
        "\n",
        "# Load and preprocess your text data\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/My Drive/CTTC/RNN-data/all_tswift_lyrics.txt'\n",
        "path = '/content/drive/My Drive/CTTC/RNN-data/NosotrosEnLaLuna.txt'\n",
        "data = open(path).read().lower()\n",
        "print('length of the corpus is:', len(data))\n",
        "\n",
        "# Create a tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "encoded_data = tokenizer.texts_to_sequences([data])[0]\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Create input sequences and corresponding labels\n",
        "sequence_length = 2  # Adjust this based on your desired sequence length\n",
        "sequences = []\n",
        "for i in range(sequence_length, len(encoded_data)):\n",
        "    sequence = encoded_data[i - sequence_length:i + 1]\n",
        "    sequences.append(sequence)\n",
        "sequences = torch.LongTensor(sequences)  # Convert to PyTorch tensor\n",
        "\n",
        "# Separate into input (X) and target (y) tensors\n",
        "X = sequences[:, :-1]  # All except the last element\n",
        "y = sequences[:, -1]    # Last element\n",
        "\n",
        "# Create DataLoader for training and validation\n",
        "dataset = TensorDataset(X, y)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "batch_size = 32\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "N4eS-kEdqQB-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab862321-8df9-4877-c60d-39b8d8a78b7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "length of the corpus is: 656424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for data, labels in iter(train_dataloader):\n",
        "  print(data.size())\n",
        "  print(labels)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhcxW8902jwm",
        "outputId": "c4ff8c2a-0242-4304-fde8-d6aa6d3c2a8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 2])\n",
            "tensor([2733,   54,  110,    8,  924, 1828,   10,   57,   69,    4, 1182,    7,\n",
            "          98,    8,   17,   35,  963,  821,  931, 4374, 2121, 5477,  233,   48,\n",
            "          92,  219,    8,   27, 1868,    2,   11,  237])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhxEJIRK2mB6",
        "outputId": "12c5f0d8-76c4-4ba9-8d47-bd7985333701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([270,   5,   3, 181,  14])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recurrent SNN"
      ],
      "metadata": {
        "id": "o74BVLN48MA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Network\n",
        "class RSNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # hyperparam stuff\n",
        "        num_inputs = 2\n",
        "        num_hidden = 512\n",
        "        beta = 0.9 # arbitrary\n",
        "\n",
        "        # Initialize layers\n",
        "        self.fc1 = nn.Linear(num_inputs, num_hidden).to(torch.float32)\n",
        "        self.lif1 = snn.RLeaky(beta=beta, linear_features=num_hidden).to(torch.float32) # RLeaky instead or Leaky\n",
        "\n",
        "        self.fc2 = nn.Linear(num_hidden, vocab_size).to(torch.float32)\n",
        "        self.lif2 = snn.RLeaky(beta=beta, linear_features=vocab_size).to(torch.float32)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Initialize hidden states at t=0\n",
        "        spk1, mem1 = self.lif1.init_rleaky()\n",
        "        spk2, mem2 = self.lif2.init_rleaky()\n",
        "\n",
        "        # Record the final layer\n",
        "        spk2_rec = []\n",
        "        mem2_rec = []\n",
        "\n",
        "        for step in range(x.size(0)):  # time x batch x num_inputs\n",
        "            # cur1 = self.fc1(x[step].flatten(1))\n",
        "            cur1 = self.fc1(x[step])\n",
        "            spk1, mem1 = self.lif1(cur1, spk1, mem1)\n",
        "            cur2 = self.fc2(spk1)\n",
        "            spk2, mem2 = self.lif2(cur2, spk2, mem2)\n",
        "            spk2_rec.append(spk2)\n",
        "            mem2_rec.append(mem2)\n",
        "\n",
        "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
        "\n",
        "\n",
        "# Load the network onto CUDA if available\n",
        "rnet = RSNN().to(device)"
      ],
      "metadata": {
        "id": "S2RYJkwe8OxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(rnet.parameters(), lr=1e-3, betas=(0.9, 0.999))\n",
        "\n",
        "num_epochs = 5\n",
        "loss_hist = []\n",
        "counter = 0\n",
        "\n",
        "# Outer training loop\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"Epoch\", epoch)\n",
        "    counter = 0\n",
        "    # Minibatch training loop\n",
        "    for data, targets in iter(train_dataloader):\n",
        "        data = data.to(device, dtype=torch.float32)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # forward pass\n",
        "        rnet.train()\n",
        "\n",
        "        spk_rec, _ = rnet(data.unsqueeze(0))\n",
        "\n",
        "        # initialize the loss & sum over time\n",
        "        # loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "        # loss_val += loss(spk_rec.sum(0), targets)\n",
        "        loss_val = loss(spk_rec.view(-1, vocab_size), targets.view(-1))  # Reshape for CrossEntropyLoss\n",
        "\n",
        "        # Gradient calculation + weight update\n",
        "        optimizer.zero_grad()\n",
        "        loss_val.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Store loss history for future plotting\n",
        "        loss_hist.append(loss_val.item())\n",
        "\n",
        "        # Print train/test loss/accuracy\n",
        "        if counter % 10 == 0:\n",
        "            print(f\"Iteration: {counter} \\t Train Loss: {loss_val.item()}\")\n",
        "        counter += 1\n",
        "\n",
        "        if counter == 100:\n",
        "          break"
      ],
      "metadata": {
        "id": "Z02Yf8mc8RXA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "444c2aac-58d4-4832-9a7a-00182979979e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "Iteration: 0 \t Train Loss: 9.36059284210205\n",
            "Iteration: 10 \t Train Loss: 9.197749137878418\n",
            "Iteration: 20 \t Train Loss: 9.13675308227539\n",
            "Iteration: 30 \t Train Loss: 8.981857299804688\n",
            "Iteration: 40 \t Train Loss: 9.201375961303711\n",
            "Iteration: 50 \t Train Loss: 9.045947074890137\n",
            "Iteration: 60 \t Train Loss: 9.015724182128906\n",
            "Iteration: 70 \t Train Loss: 9.016396522521973\n",
            "Iteration: 80 \t Train Loss: 9.048698425292969\n",
            "Iteration: 90 \t Train Loss: 8.830820083618164\n",
            "Epoch 1\n",
            "Iteration: 0 \t Train Loss: 8.831679344177246\n",
            "Iteration: 10 \t Train Loss: 8.644898414611816\n",
            "Iteration: 20 \t Train Loss: 8.9275484085083\n",
            "Iteration: 30 \t Train Loss: 8.897180557250977\n",
            "Iteration: 40 \t Train Loss: 8.741438865661621\n",
            "Iteration: 50 \t Train Loss: 8.929606437683105\n",
            "Iteration: 60 \t Train Loss: 8.804450035095215\n",
            "Iteration: 70 \t Train Loss: 9.086922645568848\n",
            "Iteration: 80 \t Train Loss: 9.087335586547852\n",
            "Iteration: 90 \t Train Loss: 8.838024139404297\n",
            "Epoch 2\n",
            "Iteration: 0 \t Train Loss: 8.807150840759277\n",
            "Iteration: 10 \t Train Loss: 8.838738441467285\n",
            "Iteration: 20 \t Train Loss: 8.808237075805664\n",
            "Iteration: 30 \t Train Loss: 9.090022087097168\n",
            "Iteration: 40 \t Train Loss: 8.80943775177002\n",
            "Iteration: 50 \t Train Loss: 8.83970832824707\n",
            "Iteration: 60 \t Train Loss: 8.903279304504395\n",
            "Iteration: 70 \t Train Loss: 8.778501510620117\n",
            "Iteration: 80 \t Train Loss: 8.747690200805664\n",
            "Iteration: 90 \t Train Loss: 8.873987197875977\n",
            "Epoch 3\n",
            "Iteration: 0 \t Train Loss: 8.905980110168457\n",
            "Iteration: 10 \t Train Loss: 8.686517715454102\n",
            "Iteration: 20 \t Train Loss: 8.905448913574219\n",
            "Iteration: 30 \t Train Loss: 8.874707221984863\n",
            "Iteration: 40 \t Train Loss: 8.907158851623535\n",
            "Iteration: 50 \t Train Loss: 8.8751802444458\n",
            "Iteration: 60 \t Train Loss: 8.751164436340332\n",
            "Iteration: 70 \t Train Loss: 8.751321792602539\n",
            "Iteration: 80 \t Train Loss: 8.689006805419922\n",
            "Iteration: 90 \t Train Loss: 8.909804344177246\n",
            "Epoch 4\n",
            "Iteration: 0 \t Train Loss: 8.940223693847656\n",
            "Iteration: 10 \t Train Loss: 8.815497398376465\n",
            "Iteration: 20 \t Train Loss: 8.690328598022461\n",
            "Iteration: 30 \t Train Loss: 8.909785270690918\n",
            "Iteration: 40 \t Train Loss: 8.8482666015625\n",
            "Iteration: 50 \t Train Loss: 8.81664752960205\n",
            "Iteration: 60 \t Train Loss: 8.785988807678223\n",
            "Iteration: 70 \t Train Loss: 8.848779678344727\n",
            "Iteration: 80 \t Train Loss: 8.942402839660645\n",
            "Iteration: 90 \t Train Loss: 8.850261688232422\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to evaluation mode\n",
        "rnet.eval()\n",
        "\n",
        "# Define your input sequence as a list of word indices (adjust this based on your input)\n",
        "input_sequence = [tokenizer.word_index[\"mi\"], tokenizer.word_index[\"amigo\"]]  # Example input, adjust as needed\n",
        "\n",
        "# Convert the input sequence to a tensor\n",
        "input_sequence = torch.tensor(input_sequence, dtype=torch.float32).to(device)\n",
        "\n",
        "# Run inference on the input sequence\n",
        "with torch.no_grad():\n",
        "    output_spikes, _ = rnet(input_sequence.unsqueeze(0).unsqueeze(0))\n",
        "\n",
        "# Convert output spikes into probabilities\n",
        "output_probabilities = torch.softmax(output_spikes[-1], dim=1)\n",
        "print(output_probabilities)\n",
        "\n",
        "# Get the predicted word index (argmax)\n",
        "predicted_word_index = output_probabilities.argmax(dim=1).item()\n",
        "\n",
        "# Convert the predicted word index back to text using your vocabulary mapping\n",
        "predicted_word = tokenizer.index_word[predicted_word_index]\n",
        "\n",
        "# You now have the generated word based on your model's prediction\n",
        "print(predicted_word_index, \"-->\", predicted_word)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH-aWdW4D21Q",
        "outputId": "74e2cb1d-a8e7-4137-9d39-af8f0f269c90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[8.1919e-05, 2.2268e-04, 2.2268e-04,  ..., 8.1919e-05, 8.1919e-05,\n",
            "         8.1919e-05]])\n",
            "1 --> que\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_printoptions(threshold=10_000)"
      ],
      "metadata": {
        "id": "2uCwBR46MizM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}