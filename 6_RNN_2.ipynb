{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexuqt/neural-networks/blob/main/6_RNN_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzPw3FkGu6YQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import array\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7P2eUDovCXW"
      },
      "outputs": [],
      "source": [
        "# source text\n",
        "data = \"I am not the kind of girl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aiv3Nn-H7apl",
        "outputId": "fd62e24f-7b23-4e84-bd64-999986ff2588"
      },
      "outputs": [
        {
          "ename": "MessageError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a4b1aecc65bb>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# path = '/content/drive/My Drive/CTTC/1661-0.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/CTTC/RNN-data/all_tswift_lyrics.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# path = '/content/drive/My Drive/CTTC/RNN-data/NosotrosEnLaLuna.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# path = '/content/drive/My Drive/CTTC/1661-0.txt'\n",
        "path = '/content/drive/My Drive/CTTC/RNN-data/all_tswift_lyrics.txt'\n",
        "# path = '/content/drive/My Drive/CTTC/RNN-data/NosotrosEnLaLuna.txt'\n",
        "data = open(path).read().lower()\n",
        "print('length of the corpus is: :', len(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1U81aiOdvDQS"
      },
      "outputs": [],
      "source": [
        "# integer encode text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "encoded_data = tokenizer.texts_to_sequences([data])[0]\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1  # 0 is reserved for padding so that's why we added 1\n",
        "\n",
        "print('Tokenizer/Dictionary size (0 padding included):', vocab_size, word_index)\n",
        "print(\"Encoded data:\", encoded_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QcJ50lS0vDNN"
      },
      "outputs": [],
      "source": [
        "# create word -> word sequences\n",
        "sequences = list()\n",
        "for i in range(1, len(encoded_data)):\n",
        "\tsequence = encoded_data[i-1:i+1]\n",
        "\tsequences.append(sequence)\n",
        "print('Total sequences:', len(sequences))\n",
        "\n",
        "for sequence in sequences[:5]:\n",
        "  decoded_words = [tokenizer.index_word[idx] for idx in sequence]\n",
        "  # decoded_text = ' '.join(decoded_words) # passa de ['you', \"don't\"] a \"you don't\"\n",
        "  print(decoded_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Pd0thnJOvDIN"
      },
      "outputs": [],
      "source": [
        "# split sequences into input (X) and output (y)\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,0],sequences[:,1]\n",
        "print(X[:5])\n",
        "print(y[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pVqC0u4tvDF2"
      },
      "outputs": [],
      "source": [
        "# one hot encode outputs\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "# define model\n",
        "y[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NXsGlDGdvDDF"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=1))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kW7SNqbcvDAu"
      },
      "outputs": [],
      "source": [
        "# compile network\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vlRLzZFevC-G"
      },
      "outputs": [],
      "source": [
        "# fit network\n",
        "start_time = time.time()\n",
        "\n",
        "history = model.fit(X, y, epochs=30)\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"It took {elapsed_time:.2f} seconds to train the model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NNjK7j1xiE1a"
      },
      "outputs": [],
      "source": [
        "model.save('ts.h5')\n",
        "pickle.dump(history, open(\"history.p\", \"wb\"))\n",
        "model = load_model('ts.h5')\n",
        "history = pickle.load(open(\"history.p\", \"rb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SjV-ZE0_hfB5"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))  # Create a larger figure to accommodate both plots\n",
        "\n",
        "# Plot for model accuracy\n",
        "plt.subplot(1, 2, 1)  # Create subplot 1 (rows, columns, index)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "# Plot for model loss\n",
        "plt.subplot(1, 2, 2)  # Create subplot 2 (rows, columns, index)\n",
        "plt.plot(history.history['loss'], color='orange')  # Use a different color for distinction\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "plt.tight_layout()  # Adjust spacing between subplots for better layout\n",
        "plt.show()  # Display the combined plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYqQPRphSRLK"
      },
      "source": [
        "### **Option 1: get a phrase adding N words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "k0gd6aqHvC7n"
      },
      "outputs": [],
      "source": [
        "def generate_seq(model, tokenizer, enter_text, n_pred):\n",
        "    index = tokenizer.word_index.get(enter_text.lower())\n",
        "    if index is not None:\n",
        "      in_text, result = enter_text, enter_text\n",
        "\n",
        "      # Generate a fixed number of words\n",
        "      for _ in range(n_pred):\n",
        "        # Encode the text as integers\n",
        "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        encoded = np.array(encoded)\n",
        "\n",
        "        # Predict probabilities for each word in the vocabulary\n",
        "        yhat_probs = model.predict(encoded.reshape(1, -1), verbose=0)[0]\n",
        "\n",
        "        # Get the index of the predicted word\n",
        "        yhat_idx = np.argmax(yhat_probs)\n",
        "\n",
        "        # Map predicted word index to word\n",
        "        out_word = tokenizer.index_word.get(yhat_idx, '')  # Get the word if it exists, otherwise use an empty string\n",
        "\n",
        "        # Append to input\n",
        "        in_text, result = out_word, result + ' ' + out_word\n",
        "\n",
        "      return result\n",
        "    else:\n",
        "        return f\"'{enter_text}' is not in the tokenizer's vocabulary\"\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cytHPG-FvC4_"
      },
      "outputs": [],
      "source": [
        "# evaluate\n",
        "print(generate_seq(model, tokenizer, 'this', 13))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KneVPTFSBY2"
      },
      "source": [
        "### **Option 2: get next N words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xRG58wziKOul"
      },
      "outputs": [],
      "source": [
        "def get_next(model, tokenizer, enter_text, top_n=3):\n",
        "    index = tokenizer.word_index.get(enter_text.lower())\n",
        "    if index is not None:\n",
        "      in_text, result = enter_text, enter_text\n",
        "\n",
        "      # Encode the text as integers\n",
        "      encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "      encoded = np.array(encoded)\n",
        "\n",
        "      # Predict probabilities for each word in the vocabulary\n",
        "      yhat_probs = model.predict(encoded.reshape(1, -1), verbose=0)[0]\n",
        "\n",
        "      top_indices = np.argsort(-yhat_probs)[:top_n]\n",
        "\n",
        "      for position, idx in enumerate(top_indices, start=1):\n",
        "        print(f\"{position:<3} {tokenizer.index_word[idx]:<12} {yhat_probs[idx]*100:.1f}%\")\n",
        "\n",
        "\n",
        "      decoded_words = [tokenizer.index_word[idx] for idx in top_indices]\n",
        "      return decoded_words\n",
        "    else:\n",
        "        return f\"'{enter_text}' is not in the tokenizer's vocabulary\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wFVm00zHvC0N"
      },
      "outputs": [],
      "source": [
        "get_next(model, tokenizer, 'why', 3);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DErs6QfSgEe"
      },
      "source": [
        "### **Option 3: build a phrase choosing between N words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mzJ_mKCfSgmd"
      },
      "outputs": [],
      "source": [
        "def build_phrase(model, tokenizer, enter_text, top_n=3):\n",
        "  index = tokenizer.word_index.get(enter_text.lower())\n",
        "  if index is not None:\n",
        "    in_text, result = enter_text, enter_text\n",
        "\n",
        "    quit = False\n",
        "    print(\"Press 'q' to quit\")\n",
        "    while not quit:\n",
        "      options = get_next(model, tokenizer, in_text, top_n)\n",
        "      while True:\n",
        "        choice = input(f\"Please enter a number between 1 and {top_n}: \")\n",
        "        if(choice=='q'):\n",
        "          quit = True\n",
        "          break\n",
        "        if choice.isdigit():\n",
        "          chosen_index = int(choice)\n",
        "          if 1 <= chosen_index <= top_n:\n",
        "            break\n",
        "          else:\n",
        "              print(f\"Number is not in the range 1-{top_n}. Please try again.\")\n",
        "\n",
        "      out_word = options[chosen_index-1]\n",
        "\n",
        "      in_text = out_word\n",
        "      if not quit:\n",
        "        result += ' ' + out_word\n",
        "      print(\"-----\")\n",
        "      print(result)\n",
        "      print(\"-----\")\n",
        "  else:\n",
        "    return f\"'{enter_text}' is not in the tokenizer's vocabulary\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "z9BEcNVoWIK9"
      },
      "outputs": [],
      "source": [
        "build_phrase(model, tokenizer, 'hola', 3)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpCqmcNyCp2wUzRXzSq2W2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}